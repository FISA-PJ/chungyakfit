pipeline {
    agent any
    
    environment {
        // GitHub ë¦¬í¬ì§€í† ë¦¬ ì„¤ì •
        GITHUB_CREDENTIALS_ID = 'github-token'
        GITHUB_REPO = 'https://github.com/FISA-PJ/SpringBackEnd.git'
        GITHUB_BRANCH = 'main'
        
        // Airflow ì„œë²„ ì„¤ì • (ì™„ì „ ë¶„ë¦¬)
        AIRFLOW_HOST_ID = 'airflow-host'           // ap.loclx.io
        AIRFLOW_USER_ID = 'airflow-user'           // ubuntu
        AIRFLOW_PORT_ID = 'airflow-port'           // 22223
        SSH_KEY_ID = 'airflow-ssh-key'             // Airflow ì„œë²„ SSH í‚¤
        
        // ë°°í¬ ê²½ë¡œ ì„¤ì •
        LOCAL_DAG_DIR = 'airflow/dags/'            // ë¡œì»¬ DAG ë””ë ‰í† ë¦¬
        REMOTE_DAG_PATH_ID = 'airflow-dags-path'   // ì›ê²© DAG ê²½ë¡œ
    }
    
    triggers {
        githubPush()  // GitHub push ì‹œ ìë™ íŠ¸ë¦¬ê±°
    }
    
    stages {
        stage('Checkout DAGs') {
            steps {
                script {
                    echo "Checking out DAGs from GitHub..."
                    
                    // GitHubì—ì„œ DAG ì½”ë“œ ì²´í¬ì•„ì›ƒ
                    git branch: "${GITHUB_BRANCH}", 
                        url: "${GITHUB_REPO}",
                        credentialsId: "${GITHUB_CREDENTIALS_ID}"
                    
                    // DAG ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
                    sh """
                        if [ -d "${LOCAL_DAG_DIR}" ]; then
                            echo "DAG ë””ë ‰í† ë¦¬ ë°œê²¬: ${LOCAL_DAG_DIR}"
                            ls -la ${LOCAL_DAG_DIR}
                        else
                            echo "âš ï¸  DAG ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: ${LOCAL_DAG_DIR}"
                            mkdir -p ${LOCAL_DAG_DIR}
                        fi
                    """
                }
            }
        }
        
        stage('Validate DAG Files') {
            steps {
                script {
                    echo "DAG íŒŒì¼ ê²€ì¦ ì¤‘..."
                    
                    // Python ë¬¸ë²• ê²€ì‚¬ (ì„ íƒì‚¬í•­)
                    sh """
                        # DAG íŒŒì¼ë“¤ì˜ Python ë¬¸ë²• ê²€ì‚¬
                        find ${LOCAL_DAG_DIR} -name "*.py" -type f | while read file; do
                            echo "Checking syntax: \$file"
                            python3 -m py_compile "\$file" || echo "âš ï¸  Syntax error in \$file"
                        done
                    """
                }
            }
        }
        
        stage('Deploy DAGs to Airflow Server') {
            steps {
                withCredentials([
                    string(credentialsId: "${AIRFLOW_HOST_ID}", variable: 'AIRFLOW_HOST'),
                    string(credentialsId: "${AIRFLOW_USER_ID}", variable: 'AIRFLOW_USER'),
                    string(credentialsId: "${AIRFLOW_PORT_ID}", variable: 'AIRFLOW_PORT'),
                    string(credentialsId: "${REMOTE_DAG_PATH_ID}", variable: 'REMOTE_DAG_PATH')
                ]) {
                    sshagent(credentials: ["${SSH_KEY_ID}"]) {
                        script {
                            echo "Deploying DAGs to Airflow server: ${AIRFLOW_USER}@${AIRFLOW_HOST}:${AIRFLOW_PORT}"
                            echo "Target path: ${REMOTE_DAG_PATH}"
                            
                            // ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ë°°í¬
                            def maxRetries = 3
                            def deploySuccess = false
                            
                            for (int i = 0; i < maxRetries && !deploySuccess; i++) {
                                try {
                                    echo "ë°°í¬ ì‹œë„ ${i + 1}/${maxRetries}..."
                                    
                                    timeout(time: 5, unit: 'MINUTES') {
                                        sh """
                                            # SSH ì—°ê²° í…ŒìŠ¤íŠ¸ (ë¶„ë¦¬ëœ ë³€ìˆ˜ ì‚¬ìš©)
                                            ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 -p \${AIRFLOW_PORT} \${AIRFLOW_USER}@\${AIRFLOW_HOST} 'echo "SSH ì—°ê²° ì„±ê³µ"'
                                            
                                            # ì›ê²© ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
                                            ssh -o StrictHostKeyChecking=no -p \${AIRFLOW_PORT} \${AIRFLOW_USER}@\${AIRFLOW_HOST} 'mkdir -p \${REMOTE_DAG_PATH}'
                                            
                                            # DAG íŒŒì¼ë“¤ì„ Airflow ì„œë²„ë¡œ ë³µì‚¬ (ë¶„ë¦¬ëœ ë³€ìˆ˜ ì‚¬ìš©)
                                            rsync -avz --delete \\
                                                -e 'ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 -p \${AIRFLOW_PORT}' \\
                                                \${LOCAL_DAG_DIR} \\
                                                \${AIRFLOW_USER}@\${AIRFLOW_HOST}:\${REMOTE_DAG_PATH}/
                                            
                                            # ë°°í¬ ê²°ê³¼ í™•ì¸
                                            ssh -o StrictHostKeyChecking=no -p \${AIRFLOW_PORT} \${AIRFLOW_USER}@\${AIRFLOW_HOST} \\
                                                'ls -la \${REMOTE_DAG_PATH} && echo "ë°°í¬ëœ DAG íŒŒì¼ ìˆ˜: \$(find \${REMOTE_DAG_PATH} -name "*.py" | wc -l)"'
                                        """
                                    }
                                    
                                    deploySuccess = true
                                    echo "âœ… DAG ë°°í¬ ì„±ê³µ!"
                                    
                                } catch (Exception e) {
                                    echo "âŒ ë°°í¬ ì‹œë„ ${i + 1} ì‹¤íŒ¨: ${e.getMessage()}"
                                    if (i < maxRetries - 1) {
                                        echo "30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„..."
                                        sleep(30)
                                    } else {
                                        error("DAG ë°°í¬ê°€ ${maxRetries}ë²ˆ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        
        stage('Restart Airflow (Optional)') {
            steps {
                withCredentials([
                    string(credentialsId: "${AIRFLOW_HOST_ID}", variable: 'AIRFLOW_HOST'),
                    string(credentialsId: "${AIRFLOW_USER_ID}", variable: 'AIRFLOW_USER'),
                    string(credentialsId: "${AIRFLOW_PORT_ID}", variable: 'AIRFLOW_PORT')
                ]) {
                    sshagent(credentials: ["${SSH_KEY_ID}"]) {
                        script {
                            echo "Airflow ì„œë¹„ìŠ¤ ì¬ì‹œì‘ (ì„ íƒì‚¬í•­)..."
                            
                            try {
                                timeout(time: 2, unit: 'MINUTES') {
                                    sh """
                                        ssh -o StrictHostKeyChecking=no -p \${AIRFLOW_PORT} \${AIRFLOW_USER}@\${AIRFLOW_HOST} '
                                            # Docker Compose ì‚¬ìš© ì‹œ Airflow ì¬ì‹œì‘
                                            cd /mnt/d/team3/ml-backend-fastapi/airflow
                                            
                                            # Docker ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ (ì‹¤ì œ êµ¬ì„±ì— ë§ê²Œ ìˆ˜ì •)
                                            docker-compose restart webserver scheduler || echo "Docker restart failed"
                                            
                                            # ë˜ëŠ” systemd ì‚¬ìš© ì‹œ (ì£¼ì„ í•´ì œ)
                                            # sudo systemctl restart airflow-webserver || echo "Webserver restart failed"
                                            # sudo systemctl restart airflow-scheduler || echo "Scheduler restart failed"
                                            
                                            echo "Airflow ì¬ì‹œì‘ ì™„ë£Œ"
                                        '
                                    """
                                }
                            } catch (Exception e) {
                                echo "âš ï¸  Airflow ì¬ì‹œì‘ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): ${e.getMessage()}"
                            }
                        }
                    }
                }
            }
        }
    }
    
    post {
        always {
            script {
                // ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì •ë¦¬
                sh 'ls -la'
                
                // ë¹Œë“œ ê²°ê³¼ ì•„ì¹´ì´ë¸Œ
                archiveArtifacts artifacts: "${LOCAL_DAG_DIR}**/*.py", allowEmptyArchive: true
            }
        }
        
        success {
            echo """
            ğŸ‰ DAG ë°°í¬ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!
            
            âœ… ë°°í¬ëœ ë””ë ‰í† ë¦¬: ${LOCAL_DAG_DIR}
            âœ… ëŒ€ìƒ ì„œë²„: WSL2 Airflow Server  
            âœ… ë°°í¬ ì‹œê°„: ${new Date()}
            
            DAG íŒŒì¼ë“¤ì´ WSL2 Airflow ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤.
            """
        }
        
        failure {
            echo """
            âŒ DAG ë°°í¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨!
            
            ì‹¤íŒ¨í•œ ë¹Œë“œ: ${BUILD_NUMBER}
            ì‹¤íŒ¨ ì‹œê°„: ${new Date()}
            
            ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.
            """
        }
    }
}